{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This model takes windows of assignment grades as input output a onehot vector of size 4 which indicates whether the student completed the assignment, missed the assignment, dropping CSCA08 or already dropped CSCA08 previously.(i.e. [1,0,0,0] indicates that the student completed the assignment) The accuracy score is around 0.842, and 0.427 for f1 score.\n"
      ],
      "metadata": {
        "id": "xYXX269l7Hd5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmwXwdp8_cl1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import nn,optim\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import sklearn.preprocessing as preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,recall_score,precision_score,roc_curve,roc_auc_score,accuracy_score,confusion_matrix,r2_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import torch.nn.functional as F\n",
        "from abc import ABC\n",
        "import torch.utils.data as Data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os ,re\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "M077GNwm_fq4",
        "outputId": "50fb009e-adf7-4b69-e148-bf0241d67725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Year            Id    UTorID           Type Gender Gender2  Exe0  \\\n",
              "0     2015  9.902906e+08  dcostaad       Domestic     MR    Male   1.0   \n",
              "1     2015  9.982007e+08  wangjunq  International     MR    Male   0.0   \n",
              "2     2015  9.983004e+08   xuzheng       Domestic     MR    Male   1.0   \n",
              "3     2015  9.983261e+08  leehyu37  International     MR    Male   1.0   \n",
              "4     2015  9.984079e+08  khanmu31       Domestic     MR    Male   0.0   \n",
              "...    ...           ...       ...            ...    ...     ...   ...   \n",
              "2363  2017  1.004574e+09  zhanjenk       Domestic    NaN     NaN   1.0   \n",
              "2364  2017  1.003323e+09  zhaodav3       Domestic     MR    Male   0.0   \n",
              "2365  2017  1.003779e+09  zhaoyu79  International     MS  Female   0.0   \n",
              "2366  2017  1.003557e+09  zhouwe42  International   MISS  Female   0.0   \n",
              "2367  2017  1.004445e+09  zhujeff2       Domestic    NaN     NaN   1.0   \n",
              "\n",
              "          Exe1      Exe2      Exe3  ...       TT1       TT2    EX  EX.1  \\\n",
              "0     1.000000  0.818182  1.000000  ...  0.816667  0.800000  0.67  0.67   \n",
              "1     1.000000  0.909091  1.000000  ...  0.566667  0.466667  0.43  0.43   \n",
              "2     1.000000  0.818182  1.000000  ...  0.658333  0.733333  0.56  0.56   \n",
              "3     0.666667  0.818182  0.916667  ...  0.583333  0.866667  0.84  0.84   \n",
              "4     1.000000  0.545455  1.000000  ...  0.541667  0.400000  0.37  0.37   \n",
              "...        ...       ...       ...  ...       ...       ...   ...   ...   \n",
              "2363  1.000000  0.714000  0.737000  ...       NaN       NaN   NaN   NaN   \n",
              "2364       NaN       NaN       NaN  ...       NaN       NaN   NaN   NaN   \n",
              "2365       NaN       NaN       NaN  ...       NaN       NaN   NaN   NaN   \n",
              "2366       NaN       NaN       NaN  ...       NaN       NaN   NaN   NaN   \n",
              "2367       NaN       NaN       NaN  ...  0.222222       NaN   NaN   NaN   \n",
              "\n",
              "      FinalGrade    Status  DropTime  PartialGrade  Assignment Partial  \\\n",
              "0       0.774253  Complete       NaN      0.868835            0.908310   \n",
              "1       0.559584  Complete       NaN      0.641919            0.776137   \n",
              "2       0.657379  Complete       NaN      0.812951            0.914187   \n",
              "3       0.837111  Complete       NaN      0.801598            0.863400   \n",
              "4       0.394412  Complete       NaN      0.552943            0.775221   \n",
              "...          ...       ...       ...           ...                 ...   \n",
              "2363         NaN       EX5    before           NaN                 NaN   \n",
              "2364         NaN       EX1    before           NaN                 NaN   \n",
              "2365         NaN       EX1    before           NaN                 NaN   \n",
              "2366         NaN       EX1    before           NaN                 NaN   \n",
              "2367         NaN       EX1    before           NaN                 NaN   \n",
              "\n",
              "      Mid Term Partial  \n",
              "0             0.808333  \n",
              "1             0.516667  \n",
              "2             0.695833  \n",
              "3             0.725000  \n",
              "4             0.470833  \n",
              "...                ...  \n",
              "2363               NaN  \n",
              "2364               NaN  \n",
              "2365               NaN  \n",
              "2366               NaN  \n",
              "2367               NaN  \n",
              "\n",
              "[2368 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-136ee353-6b03-48a0-ad41-1ec49e747f45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Id</th>\n",
              "      <th>UTorID</th>\n",
              "      <th>Type</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Gender2</th>\n",
              "      <th>Exe0</th>\n",
              "      <th>Exe1</th>\n",
              "      <th>Exe2</th>\n",
              "      <th>Exe3</th>\n",
              "      <th>...</th>\n",
              "      <th>TT1</th>\n",
              "      <th>TT2</th>\n",
              "      <th>EX</th>\n",
              "      <th>EX.1</th>\n",
              "      <th>FinalGrade</th>\n",
              "      <th>Status</th>\n",
              "      <th>DropTime</th>\n",
              "      <th>PartialGrade</th>\n",
              "      <th>Assignment Partial</th>\n",
              "      <th>Mid Term Partial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015</td>\n",
              "      <td>9.902906e+08</td>\n",
              "      <td>dcostaad</td>\n",
              "      <td>Domestic</td>\n",
              "      <td>MR</td>\n",
              "      <td>Male</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.774253</td>\n",
              "      <td>Complete</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.868835</td>\n",
              "      <td>0.908310</td>\n",
              "      <td>0.808333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015</td>\n",
              "      <td>9.982007e+08</td>\n",
              "      <td>wangjunq</td>\n",
              "      <td>International</td>\n",
              "      <td>MR</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.559584</td>\n",
              "      <td>Complete</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.641919</td>\n",
              "      <td>0.776137</td>\n",
              "      <td>0.516667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015</td>\n",
              "      <td>9.983004e+08</td>\n",
              "      <td>xuzheng</td>\n",
              "      <td>Domestic</td>\n",
              "      <td>MR</td>\n",
              "      <td>Male</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.657379</td>\n",
              "      <td>Complete</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.812951</td>\n",
              "      <td>0.914187</td>\n",
              "      <td>0.695833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015</td>\n",
              "      <td>9.983261e+08</td>\n",
              "      <td>leehyu37</td>\n",
              "      <td>International</td>\n",
              "      <td>MR</td>\n",
              "      <td>Male</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.837111</td>\n",
              "      <td>Complete</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.801598</td>\n",
              "      <td>0.863400</td>\n",
              "      <td>0.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015</td>\n",
              "      <td>9.984079e+08</td>\n",
              "      <td>khanmu31</td>\n",
              "      <td>Domestic</td>\n",
              "      <td>MR</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.394412</td>\n",
              "      <td>Complete</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.552943</td>\n",
              "      <td>0.775221</td>\n",
              "      <td>0.470833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2363</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.004574e+09</td>\n",
              "      <td>zhanjenk</td>\n",
              "      <td>Domestic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.714000</td>\n",
              "      <td>0.737000</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EX5</td>\n",
              "      <td>before</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2364</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.003323e+09</td>\n",
              "      <td>zhaodav3</td>\n",
              "      <td>Domestic</td>\n",
              "      <td>MR</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EX1</td>\n",
              "      <td>before</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2365</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.003779e+09</td>\n",
              "      <td>zhaoyu79</td>\n",
              "      <td>International</td>\n",
              "      <td>MS</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EX1</td>\n",
              "      <td>before</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2366</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.003557e+09</td>\n",
              "      <td>zhouwe42</td>\n",
              "      <td>International</td>\n",
              "      <td>MISS</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EX1</td>\n",
              "      <td>before</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2367</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.004445e+09</td>\n",
              "      <td>zhujeff2</td>\n",
              "      <td>Domestic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EX1</td>\n",
              "      <td>before</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2368 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-136ee353-6b03-48a0-ad41-1ec49e747f45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-136ee353-6b03-48a0-ad41-1ec49e747f45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-136ee353-6b03-48a0-ad41-1ec49e747f45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df = pd.read_excel('2015-2017 CSCA08 Student Data.xlsx')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "PA-V8GrI_5vv",
        "outputId": "8942b2cf-17b5-4e6b-e641-863b78810352"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Exe0      Exe1       TT1      Exe2      Exe3        A1      Exe4  Exe5  \\\n",
              "0      1.0  1.000000  0.816667  0.818182  1.000000  0.860714  1.000000   1.0   \n",
              "1      0.0  1.000000  0.566667  0.909091  1.000000  0.767857  0.933333   1.0   \n",
              "2      1.0  1.000000  0.658333  0.818182  1.000000  0.910714  1.000000   1.0   \n",
              "3      1.0  0.666667  0.583333  0.818182  0.916667  0.878571  1.000000   1.0   \n",
              "4      0.0  1.000000  0.541667  0.545455  1.000000  0.846429  0.000000   0.3   \n",
              "...    ...       ...       ...       ...       ...       ...       ...   ...   \n",
              "2363   1.0  1.000000  0.000000  0.714000  0.737000  0.344000  0.938000   0.0   \n",
              "2364   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
              "2365   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
              "2366   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
              "2367   1.0  0.000000  0.222222  0.000000  0.000000  0.000000  0.000000   0.0   \n",
              "\n",
              "          Exe6       TT2  Exe7        A2      Exe8      Exe9     Exe10  \\\n",
              "0     1.000000  0.800000   1.0  0.942308  0.909091  1.000000  0.666667   \n",
              "1     0.545455  0.466667   0.0  0.782051  0.818182  0.941176  0.500000   \n",
              "2     1.000000  0.733333   1.0  0.916667  0.545455  0.764706  0.000000   \n",
              "3     1.000000  0.866667   1.0  0.852564  0.363636  1.000000  0.000000   \n",
              "4     0.000000  0.400000   0.0  0.724359  0.181818  0.000000  0.000000   \n",
              "...        ...       ...   ...       ...       ...       ...       ...   \n",
              "2363  0.000000  0.000000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
              "2364  0.000000  0.000000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
              "2365  0.000000  0.000000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
              "2366  0.000000  0.000000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
              "2367  0.000000  0.000000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "            A3    EX  \n",
              "0     0.737500  0.67  \n",
              "1     0.579167  0.43  \n",
              "2     0.200000  0.56  \n",
              "3     0.816667  0.84  \n",
              "4     0.187500  0.37  \n",
              "...        ...   ...  \n",
              "2363  0.000000  0.00  \n",
              "2364  0.000000  0.00  \n",
              "2365  0.000000  0.00  \n",
              "2366  0.000000  0.00  \n",
              "2367  0.000000  0.00  \n",
              "\n",
              "[2368 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7b7f235-c57b-4c1e-95c0-443f0ceaf054\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Exe0</th>\n",
              "      <th>Exe1</th>\n",
              "      <th>TT1</th>\n",
              "      <th>Exe2</th>\n",
              "      <th>Exe3</th>\n",
              "      <th>A1</th>\n",
              "      <th>Exe4</th>\n",
              "      <th>Exe5</th>\n",
              "      <th>Exe6</th>\n",
              "      <th>TT2</th>\n",
              "      <th>Exe7</th>\n",
              "      <th>A2</th>\n",
              "      <th>Exe8</th>\n",
              "      <th>Exe9</th>\n",
              "      <th>Exe10</th>\n",
              "      <th>A3</th>\n",
              "      <th>EX</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.860714</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.767857</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.782051</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.579167</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.910714</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.878571</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.852564</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.846429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.724359</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2363</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.714000</td>\n",
              "      <td>0.737000</td>\n",
              "      <td>0.344000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2364</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2365</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2366</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2367</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2368 rows × 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7b7f235-c57b-4c1e-95c0-443f0ceaf054')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7b7f235-c57b-4c1e-95c0-443f0ceaf054 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7b7f235-c57b-4c1e-95c0-443f0ceaf054');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "event_sequence = ['Exe0', 'Exe1', 'TT1', 'Exe2', 'Exe3', 'A1', 'Exe4', 'Exe5', 'Exe6', 'TT2', 'Exe7', 'A2', 'Exe8', 'Exe9', 'Exe10', 'A3', 'EX']\n",
        "df1 = df[event_sequence]\n",
        "df1 = df1.fillna(0)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l60Y6D6AEy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10309c1d-b8a0-4760-b383-0303835ec088"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2368, 17, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "g = torch.where(torch.tensor(df1.values) > 0, 1, 0).tolist()\n",
        "for i in range(len(g)):\n",
        "  if 1 in g[i]:\n",
        "    drop = len(g[i]) - g[i][::-1].index(1)\n",
        "    for j in range(len(g[i])):\n",
        "      if j < drop:\n",
        "        g[i][j] = [1,0,0,0] if g[i][j] == 1 else [0,1,0,0]\n",
        "      elif j == drop:\n",
        "        g[i][j] = [0,0,1,0]\n",
        "      else:\n",
        "        g[i][j] = [0,0,0,1]\n",
        "  else:\n",
        "    g[i] = [[0,0,0,1] for j in range(len(g[i]))]\n",
        "g = torch.Tensor(g)\n",
        "g = g.detach().numpy()\n",
        "g.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYEqikBKARjl"
      },
      "outputs": [],
      "source": [
        "history_size = 4\n",
        "target_size = 1\n",
        "x = []\n",
        "y = []\n",
        "data = df1.values\n",
        "for j in range(len(df1)):\n",
        "  for i in range(data.shape[1] - history_size):\n",
        "    x.append(data[j][i:i+history_size])\n",
        "    y.append(g[j][i+history_size:i+history_size+target_size])\n",
        "\n",
        "x = np.array(x)\n",
        "x = x.reshape([x.shape[0], x.shape[1], 1])\n",
        "y = np.array(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=90)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float)\n",
        "x_test = torch.tensor(x_test, dtype=torch.float)\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.float)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float)\n",
        "\n",
        "b1 = int(len(x_train)/15)\n",
        "b2 = int(len(x_test)/15)\n",
        "\n",
        "train_loader = Data.DataLoader(\n",
        "  dataset=Data.TensorDataset(x_train, y_train),\n",
        "  batch_size=b1,\n",
        "  shuffle=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0,\n",
        ") \n",
        "test_loader = Data.DataLoader(\n",
        "  dataset=Data.TensorDataset(x_train, y_train),\n",
        "  batch_size=b2,\n",
        "  shuffle=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0,\n",
        ")\n",
        "\n",
        "y_test = y_test.detach().numpy()\n",
        "y_train = y_train.detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHdWp0OvDa4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78478382-61dd-4e6d-a409-aaf84a2d89a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([24627, 4, 1]),\n",
              " torch.Size([6157, 4, 1]),\n",
              " (24627, 1, 4),\n",
              " (6157, 1, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLXGEDuJDk7H"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self,p,input_size, output_size, hidden_layer_size):\n",
        "      super().__init__()\n",
        "      self.hidden_layer_size = hidden_layer_size\n",
        "      self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
        "      self.linear1 = nn.Linear(hidden_layer_size*4, output_size)\n",
        "      self.dropout = nn.Dropout(p=p)\n",
        "      self.tanh = nn.Tanh()\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "  def forward(self, input_x):\n",
        "      lstm_out, _ = self.lstm(input_x)\n",
        "#      lstm_out = self.dropout(lstm_out)\n",
        "      linear_out1 = self.linear1(lstm_out.reshape(len(input_x),-1))\n",
        "      linear_out1 = self.softmax(linear_out1)\n",
        "      linear_out1 = torch.squeeze(linear_out1)\n",
        "      return linear_out1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_onehot(props):\n",
        "  assert(isinstance(props, list))\n",
        "  props = np.array(props)\n",
        "  a = np.argmax(props, axis=1)\n",
        "  output = np.zeros((len(a), props.shape[1]))\n",
        "  output[np.arange(len(a)), a] = 1\n",
        "  return output"
      ],
      "metadata": {
        "id": "lnR4c7NG4xW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POsY-GMmEPT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eade6f89-c520-4dd2-d5a2-5ebca182ae4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 0 loss_train 1.3868 loss_test 1.3817 acc_train: 0.014 acc_test 0.087 f1_train: 0.007 f1_test: 0.051\n",
            "Update model\n",
            "Epochs 1 loss_train 1.3772 loss_test 1.3716 acc_train: 0.611 acc_test 0.743 f1_train: 0.199 f1_test: 0.213\n",
            "Update model\n",
            "Epochs 2 loss_train 1.3667 loss_test 1.3603 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 3 loss_train 1.3547 loss_test 1.3471 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 4 loss_train 1.3404 loss_test 1.3311 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 5 loss_train 1.323 loss_test 1.3114 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 6 loss_train 1.3012 loss_test 1.2866 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 7 loss_train 1.2738 loss_test 1.2553 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 8 loss_train 1.2396 loss_test 1.2172 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 9 loss_train 1.2004 loss_test 1.1763 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 10 loss_train 1.1596 loss_test 1.135 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 11 loss_train 1.1205 loss_test 1.0986 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 12 loss_train 1.0882 loss_test 1.0702 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 13 loss_train 1.0642 loss_test 1.0501 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 14 loss_train 1.0477 loss_test 1.0366 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 15 loss_train 1.0367 loss_test 1.0276 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 16 loss_train 1.0293 loss_test 1.0215 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 17 loss_train 1.0242 loss_test 1.0173 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 18 loss_train 1.0207 loss_test 1.0143 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 19 loss_train 1.0181 loss_test 1.0121 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 20 loss_train 1.0162 loss_test 1.0104 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 21 loss_train 1.0148 loss_test 1.009 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 22 loss_train 1.0135 loss_test 1.0079 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 23 loss_train 1.0127 loss_test 1.007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 24 loss_train 1.0118 loss_test 1.0063 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 25 loss_train 1.011 loss_test 1.0057 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 26 loss_train 1.0106 loss_test 1.0051 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 27 loss_train 1.0101 loss_test 1.0047 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 28 loss_train 1.0097 loss_test 1.0043 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 29 loss_train 1.0095 loss_test 1.004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 30 loss_train 1.009 loss_test 1.0037 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 31 loss_train 1.0088 loss_test 1.0034 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 32 loss_train 1.0085 loss_test 1.0032 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 33 loss_train 1.0083 loss_test 1.003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 34 loss_train 1.008 loss_test 1.0028 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 35 loss_train 1.0081 loss_test 1.0027 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 36 loss_train 1.0078 loss_test 1.0025 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 37 loss_train 1.0078 loss_test 1.0024 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 38 loss_train 1.0076 loss_test 1.0023 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 39 loss_train 1.0074 loss_test 1.0022 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 40 loss_train 1.0075 loss_test 1.002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 41 loss_train 1.0074 loss_test 1.002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 42 loss_train 1.0072 loss_test 1.0019 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 43 loss_train 1.007 loss_test 1.0018 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 44 loss_train 1.0071 loss_test 1.0017 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 45 loss_train 1.007 loss_test 1.0016 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 46 loss_train 1.0069 loss_test 1.0016 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 47 loss_train 1.0069 loss_test 1.0015 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 48 loss_train 1.0068 loss_test 1.0015 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 49 loss_train 1.0067 loss_test 1.0014 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 50 loss_train 1.0067 loss_test 1.0014 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 51 loss_train 1.0066 loss_test 1.0013 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 52 loss_train 1.0065 loss_test 1.0013 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 53 loss_train 1.0066 loss_test 1.0012 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 54 loss_train 1.0065 loss_test 1.0012 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 55 loss_train 1.0065 loss_test 1.0012 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 56 loss_train 1.0064 loss_test 1.0011 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 57 loss_train 1.0065 loss_test 1.0011 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 58 loss_train 1.0064 loss_test 1.0011 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 59 loss_train 1.0063 loss_test 1.001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 60 loss_train 1.0064 loss_test 1.001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 61 loss_train 1.0062 loss_test 1.001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 62 loss_train 1.0064 loss_test 1.001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 63 loss_train 1.0062 loss_test 1.0009 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 64 loss_train 1.0063 loss_test 1.0009 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 65 loss_train 1.0063 loss_test 1.0009 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 66 loss_train 1.0062 loss_test 1.0009 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 67 loss_train 1.0062 loss_test 1.0009 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 68 loss_train 1.0062 loss_test 1.0008 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 69 loss_train 1.0061 loss_test 1.0008 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 70 loss_train 1.0062 loss_test 1.0008 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 71 loss_train 1.0061 loss_test 1.0008 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 72 loss_train 1.0062 loss_test 1.0008 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 73 loss_train 1.0061 loss_test 1.0008 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 74 loss_train 1.0062 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 75 loss_train 1.0061 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 76 loss_train 1.006 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 77 loss_train 1.0062 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 78 loss_train 1.0061 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 79 loss_train 1.0061 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 80 loss_train 1.006 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 81 loss_train 1.0059 loss_test 1.0007 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 82 loss_train 1.0059 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 83 loss_train 1.0059 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 84 loss_train 1.006 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 85 loss_train 1.0059 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 86 loss_train 1.006 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 87 loss_train 1.0059 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 88 loss_train 1.0059 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 89 loss_train 1.0059 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 90 loss_train 1.0059 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 91 loss_train 1.006 loss_test 1.0006 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 92 loss_train 1.0059 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 93 loss_train 1.0059 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 94 loss_train 1.0058 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 95 loss_train 1.0059 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 96 loss_train 1.0058 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 97 loss_train 1.0059 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 98 loss_train 1.0058 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 99 loss_train 1.0058 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 100 loss_train 1.0059 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 101 loss_train 1.0058 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 102 loss_train 1.0057 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 103 loss_train 1.0058 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 104 loss_train 1.0058 loss_test 1.0005 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 105 loss_train 1.0058 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 106 loss_train 1.0058 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 107 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 108 loss_train 1.0058 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 109 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 110 loss_train 1.0058 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 111 loss_train 1.0058 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 112 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 113 loss_train 1.0058 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 114 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 115 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 116 loss_train 1.0056 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 117 loss_train 1.0056 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 118 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 119 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 120 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 121 loss_train 1.0057 loss_test 1.0004 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 122 loss_train 1.0058 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 123 loss_train 1.0056 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 124 loss_train 1.0057 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 125 loss_train 1.0058 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 126 loss_train 1.0058 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 127 loss_train 1.0057 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 128 loss_train 1.0057 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 129 loss_train 1.0056 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 130 loss_train 1.0056 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 131 loss_train 1.0056 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 132 loss_train 1.0056 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 133 loss_train 1.0055 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 134 loss_train 1.0057 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 135 loss_train 1.0057 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 136 loss_train 1.0057 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 137 loss_train 1.0055 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 138 loss_train 1.0056 loss_test 1.0003 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 139 loss_train 1.0055 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 140 loss_train 1.0055 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 141 loss_train 1.0056 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 142 loss_train 1.0055 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 143 loss_train 1.0056 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 144 loss_train 1.0056 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 145 loss_train 1.0056 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 146 loss_train 1.0055 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 147 loss_train 1.0056 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 148 loss_train 1.0055 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 149 loss_train 1.0054 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 150 loss_train 1.0056 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 151 loss_train 1.0056 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 152 loss_train 1.0055 loss_test 1.0002 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 153 loss_train 1.0054 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 154 loss_train 1.0055 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 155 loss_train 1.0054 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 156 loss_train 1.0053 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 157 loss_train 1.0055 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 158 loss_train 1.0054 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 159 loss_train 1.0054 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 160 loss_train 1.0053 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 161 loss_train 1.0054 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 162 loss_train 1.0055 loss_test 1.0001 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 163 loss_train 1.0054 loss_test 1.0 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 164 loss_train 1.0054 loss_test 1.0 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 165 loss_train 1.0053 loss_test 1.0 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 166 loss_train 1.0053 loss_test 1.0 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 167 loss_train 1.0053 loss_test 1.0 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 168 loss_train 1.0053 loss_test 1.0 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 169 loss_train 1.0052 loss_test 0.9999 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 170 loss_train 1.0053 loss_test 0.9999 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 171 loss_train 1.0053 loss_test 0.9999 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 172 loss_train 1.0052 loss_test 0.9999 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 173 loss_train 1.0052 loss_test 0.9999 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 174 loss_train 1.0052 loss_test 0.9998 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 175 loss_train 1.0052 loss_test 0.9998 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 176 loss_train 1.0051 loss_test 0.9998 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 177 loss_train 1.0051 loss_test 0.9998 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 178 loss_train 1.0051 loss_test 0.9997 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 179 loss_train 1.0052 loss_test 0.9997 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 180 loss_train 1.005 loss_test 0.9997 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 181 loss_train 1.005 loss_test 0.9996 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 182 loss_train 1.0049 loss_test 0.9996 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 183 loss_train 1.0049 loss_test 0.9995 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 184 loss_train 1.0049 loss_test 0.9995 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 185 loss_train 1.0048 loss_test 0.9994 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 186 loss_train 1.0048 loss_test 0.9994 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 187 loss_train 1.0046 loss_test 0.9993 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 188 loss_train 1.0046 loss_test 0.9992 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 189 loss_train 1.0045 loss_test 0.9992 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 190 loss_train 1.0045 loss_test 0.9991 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 191 loss_train 1.0043 loss_test 0.999 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 192 loss_train 1.0043 loss_test 0.9988 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 193 loss_train 1.0041 loss_test 0.9987 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 194 loss_train 1.004 loss_test 0.9986 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 195 loss_train 1.0038 loss_test 0.9984 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 196 loss_train 1.0036 loss_test 0.9982 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 197 loss_train 1.0033 loss_test 0.998 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 198 loss_train 1.0032 loss_test 0.9977 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n",
            "Epochs 199 loss_train 1.0029 loss_test 0.9974 acc_train: 0.738 acc_test 0.743 f1_train: 0.212 f1_test: 0.213\n"
          ]
        }
      ],
      "source": [
        "p = 0\n",
        "l = 0.0003\n",
        "hidden_size = 16\n",
        "input_size = 1\n",
        "output_size = 4\n",
        "Epochs = 200\n",
        "model = LSTM(p, input_size, output_size, hidden_size)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=l)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n",
        "result = {}\n",
        "result['train-f1'] = []\n",
        "result['test-f1'] = []\n",
        "result['train-loss'] = []\n",
        "result['test-loss'] = []\n",
        "result['train-acc'] = []\n",
        "result['test-acc'] = []\n",
        "\n",
        "F1 = 0\n",
        "Len_train = len(train_loader)\n",
        "for epochs in range(Epochs):\n",
        "  loss_mean_train = 0\n",
        "  loss_mean_test = 0\n",
        "  acc_train = 0\n",
        "  acc_test = 0\n",
        "  f1_train = 0\n",
        "  f1_test = 0\n",
        "  model.train()\n",
        "  for data_l in train_loader:\n",
        "    seq, labels = data_l\n",
        "    # print(labels)\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(seq)\n",
        "    labels = torch.squeeze(labels)\n",
        "    single_loss = loss_function(y_pred, labels)\n",
        "    loss_mean_train += single_loss.item()\n",
        "    single_loss.backward()\n",
        "    optimizer.step()\n",
        "    # print(labels.numpy())\n",
        "    # print(softmax_onehot(y_pred.detach().tolist()))\n",
        "    f1 = f1_score(labels.numpy(), softmax_onehot(y_pred.detach().tolist()), average='macro')\n",
        "    f1_train += f1\n",
        "    acc = accuracy_score(labels.numpy(), softmax_onehot(y_pred.detach().tolist()))\n",
        "    acc_train += acc\n",
        "\n",
        "  scheduler.step()\n",
        "  loss_mean_train = loss_mean_train/Len_train\n",
        "  f1_train = f1_train/Len_train\n",
        "  acc_train = acc_train/Len_train\n",
        "  result['train-loss'].append(loss_mean_train)\n",
        "  result['train-acc'].append(acc_train)\n",
        "  result['train-f1'].append(f1_train)\n",
        "\n",
        "  model.eval()\n",
        "  y_predict = model(x_test)\n",
        "  loss_mean_test = loss_function(y_predict, torch.squeeze(torch.tensor(y_test,dtype=torch.float))).item()\n",
        "  y_predict = y_predict.detach().numpy()\n",
        "  y_test = torch.squeeze(torch.tensor(y_test, dtype=torch.float)).numpy()\n",
        "  # auc_test = roc_auc_score(y_test, y_predict)\n",
        "  f1_test = f1_score(y_test, softmax_onehot(y_predict.tolist()), average='macro')\n",
        "  acc_test = accuracy_score(y_test, softmax_onehot(y_predict.tolist()))\n",
        "  result['test-loss'].append(loss_mean_test)\n",
        "  result['test-acc'].append(acc_test)\n",
        "  result['test-f1'].append(f1_test)\n",
        "  print('Epochs',epochs,'loss_train',round(loss_mean_train,4),'loss_test',round(loss_mean_test,4),'acc_train:',round(acc_train,3),'acc_test',round(acc_test,3),'f1_train:',round(f1_train,3),'f1_test:',round(f1_test,3))\n",
        "  if F1 < f1_test:\n",
        "      F1 =f1_test\n",
        "      torch.save(model, 'model_LSTM3.pth')\n",
        "      print('Update model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jnFWZH7j7FuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhEu2kRzGQPM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "a4187c8a-90f5-4bf0-9f00-f57ba22b4f28"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result['train-loss'][:],label='Loss_train')\n",
        "plt.plot(result['test-loss'][:],label='Loss_test')\n",
        "plt.title('LOSS')\n",
        "# plt.savefig('loss_1bu1.png',dpi = 500)\n",
        "plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result['train-acc'][:],label='train_acc')\n",
        "plt.plot(result['test-acc'][:],label='test_acc')\n",
        "plt.title('acc')\n",
        "# plt.savefig('loss_1bu1.png',dpi = 500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UtdOStpfPH2"
      },
      "source": [
        "Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHhrOAaYKSzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419fcaa2-7194-450e-b8fc-e53771dda98f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "xx = [0.0,\t1.000000,\t0.1,\t0.0]\n",
        "xx = torch.tensor(xx,dtype = torch.float)\n",
        "xx = xx.reshape(1,4,1)\n",
        "xx.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgtmGETxfK0d"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrijF9EfKVv0"
      },
      "outputs": [],
      "source": [
        "model = torch.load('model_LSTM3.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt_Hok5gfosH"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCIYA_wbKWU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f756c13-b0bf-4a9a-d3db-89aedf4caa32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "y_pre = model(xx)\n",
        "torch.argmax(y_pre)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}